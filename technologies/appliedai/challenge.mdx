# lablab x AppliedAI Challenge

## Introduction to Opus
Opus is an **AI-native workflow builder** created by **AppliedAI** that turns plain-language prompts into executable workflows.

Opus offers a **visual canvas** where you can drag, drop, and link nodes‚Äîinputs/outputs, AI agents, human reviews, data connectors, decisions, and even reusable sub-workflows. Once satisfied, Opus executes them as jobs with detailed logs and support for both human and agentic reviews.

We support connecting with external platforms via **Import/Export Data**, **REST API integrations**, and **custom Python code**. You can also set up **conditional branching**, **error handling**, **metrics**, and **tracing** to keep automations understandable, testable, and easy to maintain.

---

## Challenge
Build a reusable **‚ÄúIntake ‚Üí Understand ‚Üí Decide ‚Üí Review ‚Üí Deliver‚Äù** automation that tackles the pain points of manual workflows‚Äîregardless of industry. Your solution will be an Opus workflow that keeps data and decisions traceable, handles exceptions gracefully, and uses data inputs to create actionable outputs for users.

> You may use public/demo data and free external APIs. Enterprise integrations are **not expected**.

---

## The Problem
Manual workflows share core issues across domains:

- **Messy, multi-format intake:** Data arrives via emails, PDFs, spreadsheets, images, or exports, causing bottlenecks and costly handoffs.  
- **Decisions mix rules with judgment:** Workflows often combine deterministic checks (IDs, dates, thresholds) and nuanced reasoning. Opus separates these with **Decision nodes** for rules and **Agent/Advanced Agent nodes** for reasoning.  
- **Exceptions sink time:** Edge cases (missing fields, low confidence, conflicts) slow everything down.  
- **Opaque processes:** Without audit trails, it‚Äôs difficult to track inputs, decisions, and outcomes‚Äîcreating risk and costly errors.

---

## What You‚Äôll Build
Design one **end-to-end, reusable workflow** in Opus that can be adapted to multiple industries. Demonstrate it using realistic scenarios with public/sample data.

Your automation should:
- Keep data and decisions traceable  
- Handle exceptions gracefully  
- Turn inputs into actionable outputs  

You may use **public/demo data** and **free APIs**.  
Enterprise credentials are **not required**.

---

### Required Building Blocks

#### **Data Import & Processing**
- Accept at least two input types (e.g., PDF/image, email/text, CSV/JSON, or webpage).  
- Extract and structure key fields (IDs, dates, amounts, entities).  
- Import and parse external data (file, sheet, or API).  
- Batch or paginate inputs and avoid model context limits via chunking.  
- Parallelize imports and processing where possible.

#### **Conditional Logic & Branching**
- Use at least two **Decision nodes** for auditable branching.  
- Handle errors, timeouts, and missing data gracefully.  
- Combine deterministic rules with AI reasoning.  
- Implement **multi-condition logic** (AND/OR, nesting) with transparent scores/rationales.  
- Parallelize independent steps to reduce latency.

#### **Review for Quality & Safety**
- Include at least two review checkpoints:  
  - **Agentic Review:** Auto-check outputs  
  - **Human Review:** For low-confidence or high-impact cases  
- Show how rejected items are reprocessed.

#### **Provenance & Audit**
- Produce an audit artifact (JSON or PDF) including:  
  inputs, extracted fields + confidence, rules fired, scores/rationales, review actions, timestamps, IDs, and source URLs.  
- Keep a clear decision trail; optionally stream logs to external systems.

#### **Delivery**
- Export outcomes via **email**, **Google Sheets**, or other destinations.  
- Optionally build an operator interface (web form, CLI, or API) to trigger runs, validate inputs, and retry failed items.

---

## What to Submit
1. A runnable **Opus workflow** with clear inputs/outputs  
2. A **demo video** explaining your workflow  
3. A **sample dataset** and setup notes  
4. A **short README** describing logic, reviews, and error handling  
5. **Audit artifact samples** (one accepted, one reviewed/rejected)

---

## How to Stand Out

### **Compose Opus Capabilities with Purpose**
Use key Opus nodes effectively:
- OCR/Text Extraction  
- Opus Agent  
- Decision Nodes  
- Review (Human/Agentic)  
- External Service Calls  
- Import/Export Data  
- Opus Code (custom Python)

Keep **rules and AI judgment separate**.

### **Demonstrate Real Integration Thoughtfulness**
- Normalize schemas across multiple data sources  
- Handle pagination  
- Reconcile discrepancies

### **Use Mock Data for Realism**
Mock integrations (e.g., banking, credit bureau, AML, ERP) or use free public APIs.

### **Make It Fast**
Parallelize independent tasks and include latency improvements.  
Use batching/chunking for large data.

### **Human-in-the-Loop**
Use selectively but effectively.

### **Elevate Auditability**
Produce a decision artifact linking **inputs ‚Üí checks ‚Üí decisions ‚Üí reviews**.  
Mirror logs to external stores for higher scores.

### **Lightweight Operator Experience**
Provide a UI to:
- Trigger runs  
- Validate inputs  
- Monitor progress  
- Retry failures  
- View job history  

### **Keep It Modular**
Use sub-workflows, clear naming, and parameterized policies.

### **Be Explicit About Limits**
Explain any size/context constraints and how you mitigated them.

---

## Technology

### **For Non-Technical Users**
- Use **prompt-to-workflow** generation  
- Use **visual canvas builder**

### **For Developers**
- Write **custom Python code** via the Code node  
- Integrate **external APIs** with the External Service node  
- Send/receive data via **Data Import/Export node**  
- Trigger Opus Jobs via API (see [developer.opus.com](https://developer.opus.com))

Developers can use mock API responses for external systems like ERPs, CMSs, or databases.

---

## Workflow Examples

### **Financial Services ‚Äî KYC/Onboarding**
Intake identity docs ‚Üí extract ‚Üí apply rules ‚Üí review ‚Üí audit decision.

### **Market & Social Intelligence**
Monitor sources ‚Üí dedupe/cluster ‚Üí score relevance ‚Üí enforce checks ‚Üí publish briefs.

### **People Ops ‚Äî Resume Screening**
Parse resumes ‚Üí score ‚Üí shortlist ‚Üí route ‚Üí audit trail.

### **E-Commerce Returns & Quality**
Ingest images ‚Üí assess policy compliance ‚Üí recommend action ‚Üí escalate edge cases.

### **Sales ‚Äî AI-Assisted Demos**
Profile leads ‚Üí score demo relevance ‚Üí review ‚Üí log decisions ‚Üí deliver briefs.

---

## Resources
Check the [Opus Help Center](https://help.opus.com) and onboarding process to learn the visual builder.

---

## Judgement Criteria

| Criterion | What We‚Äôre Looking For | Max Points |
|------------|------------------------|-------------|
| **Integration Depth & Breadth** | Connects to multiple systems, parallel fetching, schema normalization, clear failure handling | 25 |
| **Data Handling & Matching at Scale** | Converts unstructured ‚Üí structured, handles 100‚Äì500+ records, batching/parallelization | 25 |
| **Orchestration & Human Decisioning** | Robust branching, reviews, escalations | 25 |
| **Operability & Observability** | UI/API/CLI for monitoring, retrying, logging, audit trail | 25 |

---

## Winners

**Total Prize Pool:** üí∞ **$10,000**

üèÜ **Best Opus Workflow**  
- 1st: $3,000 + 1000 Man-Hour credits ($1,500 value)  
- 2nd: $2,000 + 500 Man-Hour credits ($750 value)  
- 3rd: $1,000 + 300 Man-Hour credits ($450 value)

üí° **Most Innovative Use of Opus:** $500 + 100 Man-Hour credits ($150 value)  
üîç **Most Auditable Workflow:** $500 + 100 Man-Hour credits ($150 value)
