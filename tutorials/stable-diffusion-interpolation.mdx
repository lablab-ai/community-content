---
title: "Stable Diffusion tutorial: How to make videos with Stable Diffusion? - Interpolation"
description: "In this tutorial I will teach you how to easily make video using interpolation process with Stable Diffusion!"
image: "https://imagedelivery.net/K11gkZF3xaVyYzFESMdWIQ/789aeaa1-b1ea-470f-ce63-681d21265f00/full"
authorUsername: jakub.misilo
---

## üßê What is Stable Diffusion?

Stable diffusion is Open Source latent text-to-image diffusion model. You can find out more [here](https://lablab.ai/tech/stable-diffusion) or try it by yourself - code is available [here](https://github.com/CompVis/stable-diffusion).

## üéØ What is our goal and how will we achieve it?

Our goal is to make a video using interpolation process. We will use the Stable Diffusion model to generate images and then we will use them to make a video. Fortunately, we don't have to write the code ourselves for interpolation between latent spaces. We will use [stable_diffusion_videos](https://github.com/nateraw/stable-diffusion-videos) library. If you want to know exactly how it works under the hood, feel free to explore the code on Github. If you need help - ask a question on the channel dedicated to this guide. You will find it on our discord!

To run this tutorial we will use [Google Colab](https://colab.research.google.com/) and [Google Drive](https://www.google.com/drive/)

## ‚öôÔ∏è Preparing dependencies

First of all, you need to install all dependencies and connect our Google Drive with Colab - to save movie and frames. You can do it by running:

```python
from google.colab import drive
drive.mount('/content/drive')
```

and then:

```bash
!pip install stable_diffusion_videos
```

Next step is authentication with Hugging Face. You can find your token [here](https://huggingface.co/settings/tokens).

```bash
!huggingface-cli login
```

## üé• Generating images/video

To generate video, we need to define prompts between which model will interpolate. We will use a dictionary for it:

```python
prompts = {
    'blonde woman': 897,
    'brunette woman': 897
}
```

Now we are able to generate images/video using:

```python
from stable_diffusion_videos import walk

walk(
    prompts=list(prompts.keys()),
    seeds=list(prompts.values()),
    output_dir='./drive/MyDrive/dir1',     # Where images/video will be saved
    name='subdir',                         # Subdirectory of output_dir where images/video will be saved
    guidance_scale=6.25,                   # Higher adheres to prompt more, lower lets model take the wheel
    num_steps=100,                         # Number of generated frames
    num_inference_steps=50,                # Number of steps by model to generate one image
    scheduler='klms',
    disable_tqdm=False,
    make_video=True,                       # If false, just save images
    use_lerp_for_text=True,
    do_loop=False,
)
```

This process may take a long time, depending on passed parameters.

There are some descriptions of parameters, but if you want to know more - check [stable_diffusion_videos](https://github.com/nateraw/stable-diffusion-videos) code! I use 100 steps between prompts, but you can use more for better results. You can also modify `num_inference_steps` and other parameters. Feel free to experiment! After running this code, you will find the video in your Google Drive. You can download it and watch and share with your friends!

If you want to reproduce my results - just copy and paste code below, but I recommend to use your own prompts and experiment with model - it's worth it!

## ‚ûï Bonus

You can use more than two prompts. Example: 

```python
prompts = {
    'blonde woman': 897,
    'brunette woman': 897,
    'ball': 10
}
```

**Thanks** for reading! Wait for next tutorials!

[Jakub Misi≈Ço](https://www.linkedin.com/in/jmisilo/) - Junior Data Scientist in [New Native](https://newnative.ai/ )
